# *****************************************************************************
# Description: Decodes .dat file generated by SIPHRA readout and stores the
# data in a .pkl or .csv file for further use.
# Written by: Mózsi Kiss & Oscar Rosero (KTH)
# ....
#   Date: 02/2026

from pathlib import Path
import argparse

from prompt_toolkit.shortcuts import input_dialog
from tqdm import tqdm
import sys
import os
import numpy as np
import pandas as pd
from d2a_decoder import D2a
from kaitaistruct import KaitaiStream, ValidationNotEqualError

pt100_calib = [(2132.45, 0.029005),
               (2342.96, 0.029048),
               (2069.69, 0.028884),
               (2171.65, 0.029113)]

crystal_id = ['A', 'B', 'C', 'D']

pt100_calib = np.asarray(pt100_calib).T


def temp(x, a, b):
    res = 100 + (x - b) * a
    return -245 + 2.3519 * res + 0.00103 * (res * res)


def process_events(f, crystal_code, subtract_baselines=False, get_external=False):
    filepath = Path(f).resolve()
    if not filepath.exists():
        raise FileNotFoundError(f'File {filepath} does not exist')
    # newpath = f'{p}/ROOT_FILES'
    # if not os.filepath.exists(newpath):
    #    os.makedirs(newpath)

    i = 0
    with open(f, 'rb') as test_file:
        while test_file.read(4) != b"\xC2\x10\x00\x00":
            i += 4
            test_file.seek(i)
            if i > 128:
                raise Exception('File is corrupted or format is invalid')
    size = os.path.getsize(f)
    num_events = int(size / 64)

    io = KaitaiStream(open(f, 'rb'))  # Open data file in streaming mode
    io.seek(i)

    data = np.empty((num_events, 23), dtype=np.uint32)
    j = 0
    while io is not None:
        try:
            data[j] = D2a.Event(io).ret
            j += 1
        except ValidationNotEqualError:
            print('seeking')
            print(io.pos())
        except EOFError:
            break

    # External HOLD assertion refers to the triggering of readout from HOLD_I
    # in the SIPHRA. Data for the baseline is obtained by asserting EXTERNAL
    # HOLD. Internal HOLD is asserted when an event is detected, i.e. when a
    # signal is received at any of the AIN (or FIN if CMIS is bypassed) inputs
    # of the SIPHRA.

    det_a_events = data[data[:, 0] == 5 + crystal_code]
    det_a_internal = det_a_events[det_a_events[:, 2] < 25] # Readouts triggered from internal HOLD, i.e. actual events
    det_a_external = det_a_events[det_a_events[:, 2] > 25] # Readouts triggered from external HOLD, i.e. baseline readouts
    # Add columns for Argmax and Summed registers
    det_a_internal = np.append(det_a_internal, np.zeros((np.shape(det_a_internal)[0], 2), dtype=np.uint32), axis=1)
    det_a_external = np.append(det_a_external, np.zeros((np.shape(det_a_external)[0], 2), dtype=np.uint32), axis=1)
    det_a_internal[:, -2] = np.argmax(det_a_internal[:, 7:-2], axis=1) + 1 # Highest-value channel

    # Calibrate temperature
    det_a_internal[:, 6] = temp(det_a_internal[:,6], pt100_calib[1, crystal_code], pt100_calib[0, crystal_code])

    if subtract_baselines:
        det_a_baselines = np.mean(det_a_external[:, 6:-2], axis=0, dtype=np.uint32)
        det_a_internal[:,6:-2] -= det_a_baselines

    # Summing performed after baseline subtraction
    det_a_internal[:, -1] = summed_channel(det_a_internal[:, 7:-2])

    dataset_internal = dataset_from_arr(det_a_internal)

    if not get_external:
        dataset_external = None
    else:
        pass
    # TODO: Implement returning the pedestal file

    return dataset_internal

def dataset_from_arr(arr: numpy.ndarray):
    '''
    Based on the event array received from d2a_decoder, store the corresponding
    information in a pandas.DataFrame for easier processing.
    Each row of the input array ´´arr´´ is one event. The array is transposed
    such that each row is a different register of all the events.
    '''
    arr_T = arr.T
    return pd.DataFrame({'Detector': arr_T[0],
                            'ID': arr_T[1],
                            'Trigger': arr_T[2],
                            'Time_sub': arr_T[3],
                            'Time_sec': arr_T[4],
                            'Time_gps': arr_T[5],
                            'Temp': arr_T[6],
                            'Ch1': arr_T[7],
                            'Ch2': arr_T[8],
                            'Ch3': arr_T[9],
                            'Ch4': arr_T[10],
                            'Ch5': arr_T[11],
                            'Ch6': arr_T[12],
                            'Ch7': arr_T[13],
                            'Ch8': arr_T[14],
                            'Ch9': arr_T[15],
                            'Ch10': arr_T[16],
                            'Ch11': arr_T[17],
                            'Ch12': arr_T[18],
                            'Ch13': arr_T[19],
                            'Ch14': arr_T[20],
                            'Ch15': arr_T[21],
                            'Ch16': arr_T[22],
                            'Argmax': arr_T[23],
                            'Summed': arr_T[24]}, dtype=np.float64)

def summed_channel(ch_values: np.ndarray) -> np.ndarray:
    '''
    Receives an array with 16 columns containing the channels readings for each event.
    Rows: events     Columns: channel reading
    '''
    # TODO: Define the summing algorithm. Currently it is just the addition of all the channels. Must be improved.
    return np.sum(ch_values, axis=1)

def build_parser():
    parser = argparse.ArgumentParser(
        prog='OR_DatToCSV',
        description='Converts .dat files obtained from SIPHRA to .csv',
        usage='%(prog)s PATH [options]',
    )
    parser.add_argument("path",
                        help="Path to .dat file or directory containing multiple .dat files",
                        type=Path,)
    parser.add_argument("--cc", "--crystal_code",
                        help="The crystal code to use. Integer between 0 and 4",
                        default=0,
                        type=int,)
    parser.add_argument("-o", "--output",
                        default='',
                        help="Path to output file")
    parser.add_argument("-v", "--verbose",
                        action="store_true",
                        help="print information about every individual file processed")
    parser.add_argument("--process_all",
                        action="store_true",
                        help="convert all .dat files in the directory, even if they already have a matching .csv file", )
    parser.add_argument("--pkl",
                        action="store_true",
                        help="Output a pkl file")
    parser.add_argument("--csv",
                        action="store_true",
                        help="Output a csv file")
    parser.add_argument("--pf", "--pedestal_file",
                        action="store_true",
                        help="In addition to the events file, output the file containing only readings triggered from external HOLD, i.e. the pedestal")
    parser.add_argument("--sb", "--subtract_baselines",
                        action="store_true",
                        help="Subtract the baseline in every channel")
    return parser

def find_lonely_dat_files(directory, suffixes=None):
    '''
    Returns a list with the paths of .dat files that have no matching .csv or
    . pkl file in the specified directory.
    :param directory: Path to directory containing .dat files
    :param suffixes: List containing the file extensions. If a .dat file in the
    directory has the same name as an existing file with extension in
    ´´suffixes´´, it will not be added to the list of files to process.
    :return: List of Paths to .dat files to be processed
    '''
    if not suffixes:
        return sorted(directory.glob('*.dat'))

    files = []
    for file in directory.glob('*.dat'):
        if not any(file.with_suffix(_).is_file() for _ in suffixes):
            files.append(file)
    return sorted(files)


if __name__ == "__main__":
    CSV = 0
    PKL = 1
    suffix_lst = ['.csv', '.pkl']

    def vprint(msg):
        if args.verbose:
            print(msg)

    def handle_outputs(data, output_suffixes, output_path):
        # print("entered handle_outputs")
        if suffix_lst[CSV] in output_suffixes:
            data.to_csv(output_path.with_suffix(suffix_lst[CSV]), index=False)
            vprint(f"Wrote {output_path.with_suffix(suffix_lst[CSV])}")
        if suffix_lst[PKL] in output_suffixes:
            data.to_pickle(output_path.with_suffix(suffix_lst[PKL]))
            vprint(f"Wrote {output_path.with_suffix(suffix_lst[PKL])}")


    args = build_parser().parse_args()

    print(args.verbose)
    vprint(f"{args.verbose=}")

    input_path = args.path.resolve()
    if not input_path.exists():
        raise FileNotFoundError(f"Path {input_path} not found!")

    # Handle file-extension and output path options
    output_suffixes = []
    output_path = Path(args.output).resolve() if args.output else input_path.with_suffix('')
    # print(f"{args.pkl=}, {args.csv=}, {output_path=}")
    # print(f"Condition evaluated: {args.pkl and not args.csv and not output_path or not output_path.suffix}, {output_path=}")
    if not args.pkl and not args.csv and not output_path.suffix:
        print("\nNo output format has been specified.\n Using default format: CSV")
        output_suffixes.append(suffix_lst[CSV])
        output_path = output_path.with_suffix(suffix_lst[CSV])
    else:
        if args.csv or output_path.suffix==suffix_lst[CSV]:
            output_suffixes.append(suffix_lst[CSV])
        if args.pkl or output_path.suffix==suffix_lst[PKL]:
            output_suffixes.append(suffix_lst[PKL])
        if not output_path.parent.is_dir():
            print("\nThe specified output folder does not exist! Defaulting to same directory as input file.\n")
            output_path = input_path.parent/output_path.stem if input_path.is_file() else input_path
    print(f"{output_suffixes=}")

    # Convert single file
    if input_path.is_file():
        vprint(f"Target file \"{input_path}\" found.")
        vprint("Processing...")
        data = process_events(input_path,
                              crystal_code=args.cc,
                              subtract_baselines=args.sb,
                              get_external=args.pf,)
        handle_outputs(data, output_suffixes, output_path)
        print(f"\nDone! 1 file converted.")

    # Convert files in a directory
    if input_path.is_dir():
        if not args.process_all:
            files = find_lonely_dat_files(input_path, output_suffixes)
            if len(files) == 0:
                sys.exit("All \'.dat\' files contain a matching \'.csv\' or \'.pkl\' file. If you want to convert all files again execute with flag --process_all")
        else:
            files = find_lonely_dat_files(input_path, None)
        qty = len(files)
        print()
        print(f"Found {qty} suitable files in directory \"{input_path.name}\".")
        print("Starting conversion...")
        for file in tqdm(files):
            vprint(f"Target file \"{input_path}\" found.")
            vprint("Processing...")
            data = process_events(input_path,
                                  crystal_code=args.cc,
                                  subtract_baselines=args.sb,
                                  get_external=args.pf, )
            handle_outputs(data, output_suffixes, file)
            vprint('')
        print(f"Done! {qty} files converted.")
    print()









